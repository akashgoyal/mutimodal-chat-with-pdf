{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import shutil\n",
    "\n",
    "#\n",
    "if os.path.exists(\"./figures\"):\n",
    "    shutil.rmtree(\"./figures\")\n",
    "os.makedirs(\"./figures\")\n",
    "\n",
    "pdf_path = '../content/echap07.pdf'\n",
    "pdf_name = os.path.basename(pdf_path)\n",
    "vs_prefix = pdf_name[:-4]\n",
    "vs_out_path = \"faiss_\"+vs_prefix\n",
    "\n",
    "img_output_dir = '../content/'+vs_prefix\n",
    "os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "if os.path.exists(vs_out_path):\n",
    "    print(f\"The directory {vs_out_path} already exists.\")\n",
    "else:\n",
    "    print(f\"The directory {vs_out_path} does not exist.\")\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy='hi_res',\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    extract_image_block_output_dir=img_output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "text_elements = []\n",
    "table_elements = []\n",
    "image_elements = []\n",
    "\n",
    "# Function to encode images\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    if 'CompositeElement' in str(type(element)):\n",
    "        text_elements.append(element)\n",
    "    elif 'Table' in str(type(element)):\n",
    "        table_elements.append(element)\n",
    "\n",
    "table_elements = [i.text for i in table_elements]\n",
    "text_elements = [i.text for i in text_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = img_output_dir\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        encoded_image = encode_image(image_path)\n",
    "        image_elements.append(encoded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "if image_elements:\n",
    "    image_data = base64.b64decode(image_elements[0])\n",
    "    display(Image(image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "\n",
    "chain_gpt_35 = ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=1024)\n",
    "chain_gpt_4_vision = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "# Function for text summaries\n",
    "def summarize_text(text_element):\n",
    "    prompt = f\"Summarize the following text:\\n\\n{text_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for table summaries\n",
    "def summarize_table(table_element):\n",
    "    prompt = f\"Summarize the following table:\\n\\n{table_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for image summaries\n",
    "def summarize_image(encoded_image):\n",
    "    prompt = [\n",
    "        AIMessage(content=\"You are a bot that is good at analyzing images.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe the contents of this image.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "    response = chain_gpt_4_vision.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries = []\n",
    "for i, te in enumerate(table_elements):\n",
    "    summary = summarize_table(te)\n",
    "    table_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = []\n",
    "for i, te in enumerate(text_elements):\n",
    "    summary = summarize_text(te)\n",
    "    text_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "def get_image_size(image_element):\n",
    "    image_data = base64.b64decode(image_element)\n",
    "    size_in_bytes = len(image_data)\n",
    "    size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "    return size_in_mb\n",
    "\n",
    "\n",
    "# Processing image elements with feedback and sleep\n",
    "image_summaries = []\n",
    "for i, ie in enumerate(image_elements):\n",
    "    summary = summarize_image(ie)\n",
    "    image_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "# Create Documents and Vectorstore\n",
    "documents = []\n",
    "retrieve_contents = []\n",
    "\n",
    "for e, s in zip(text_elements, text_summaries):\n",
    "    i = str(uuid.uuid4())\n",
    "    doc = Document(page_content=s, metadata={'id': i, 'type': 'text', 'original_content': e } )\n",
    "    retrieve_contents.append((i, e))\n",
    "    documents.append(doc)\n",
    "\n",
    "for e, s in zip(table_elements, table_summaries):\n",
    "    i = str(uuid.uuid4())\n",
    "    doc = Document(page_content=s, metadata={'id': i, 'type': 'table', 'original_content': e})\n",
    "    retrieve_contents.append((i, e))\n",
    "    documents.append(doc)\n",
    "\n",
    "for e, s in zip(image_elements, image_summaries):\n",
    "    i = str(uuid.uuid4())\n",
    "    doc = Document(page_content=s, metadata={'id': i, 'type': 'image', 'original_content': e})\n",
    "    retrieve_contents.append((i, e))\n",
    "    documents.append(doc)\n",
    "\n",
    "# Create the vector database\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"]))\n",
    "#\n",
    "vectorstore.save_local(\"faiss_\"+vs_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, Image\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import llm_init\n",
    "client = llm_init.openai_client\n",
    "\n",
    "class MyEmbeddings(Embeddings):\n",
    "    def __init__(self, client):\n",
    "        super().__init__()\n",
    "        self.client = client\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=text,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "class MyChatLLM(LLM):\n",
    "    client: OpenAI = Field(...)\n",
    "\n",
    "    def __init__(self, client):\n",
    "        super().__init__()\n",
    "        self.client = client\n",
    "\n",
    "    def _call(self, prompt, **kwargs):\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=False\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# \n",
    "embeddings = MyEmbeddings(client=client)\n",
    "mdb_chat_llm = MyChatLLM(client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "db = FAISS.load_local(\"faiss_\"+vs_prefix, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Define the prompt template for the LLMChain\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "Answer the question based only on the following context, which can include text, images, and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "Don't answer if you are not sure and decline to answer and say \"Sorry, I don't have much information about it.\"\n",
    "Just return the helpful answer in as much detail as possible.\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = LLMChain(llm=mdb_chat_llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "# Define the answer function to handle queries\n",
    "def chat_with_llm_db(question): # gpt3.5\n",
    "    relevant_docs = db.similarity_search(question)\n",
    "    context = \"\"\n",
    "    relevant_images = []\n",
    "    relevant_image_summary = []\n",
    "    for d in relevant_docs:\n",
    "        if d.metadata['type'] == 'text':\n",
    "            context += '[text]' + d.metadata['original_content']\n",
    "        elif d.metadata['type'] == 'table':\n",
    "            context += '[table]' + d.metadata['original_content']\n",
    "        elif d.metadata['type'] == 'image':\n",
    "            context += '[image]' + d.page_content\n",
    "            relevant_images.append(d.metadata['original_content'])\n",
    "            relevant_image_summary.append(d.page_content)\n",
    "    result = qa_chain.run({'context': context, 'question': question})\n",
    "    return result, relevant_images, relevant_image_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain the actions of governments in terms of forest cover, carbon storage. Answer in maximum 150 words.\"\n",
    "query = \"tell about India's National Missions\"\n",
    "result, relevant_images, relevant_image_summary = chat_with_llm_db(query)\n",
    "print(result)\n",
    "print(len(relevant_images))\n",
    "##    \n",
    "for i in range(len(relevant_images)):\n",
    "    image_data = base64.b64decode(relevant_images[i])\n",
    "    display(Image(image_data))\n",
    "    print(relevant_image_summary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
